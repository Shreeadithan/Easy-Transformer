{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29e38d58",
   "metadata": {},
   "source": [
    "# Getting Started with TransformerLens (Easy-Transformer)\n",
    "\n",
    "Welcome! This notebook will guide you through the basics of using `TransformerLens`, the successor to `Easy-Transformer`. We will cover:\n",
    "1.  **Installation**\n",
    "2.  **Loading a pre-trained model** (GPT-2 Small)\n",
    "3.  **Running the model** and accessing internal activations.\n",
    "4.  Performing a simple **ablation experiment** to understand the model's behavior.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84815774",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Installation\n",
    "# First, let's install the library. TransformerLens is the new name for Easy-Transformer.\n",
    "# The '!' allows us to run shell commands directly from the notebook.\n",
    "\n",
    "!pip install transformer-lens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c6f79",
   "metadata": {},
   "source": [
    "### 2. Loading a Model\n",
    "\n",
    "Now that the library is installed, we can import it and load a pre-trained model. We'll use `gpt2-small`, a great model for introductory interpretability work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2d0514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformer_lens\n",
    "\n",
    "# Load the GPT-2 Small model from TransformerLens\n",
    "# This automatically downloads the weights and sets up the model with all the hooks we need.\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a1ffe8",
   "metadata": {},
   "source": [
    "### 3. Running the Model & Accessing Activations\n",
    "\n",
    "Let's give the model some text and see what it predicts. The key feature of `TransformerLens` is the ability to easily cache and view all the intermediate activations inside the model. We can do this with the `run_with_cache` method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797a3ee5",
   "metadata": {},
   "source": [
    "# Sample text\n",
    "prompt = \"The quick brown fox jumps over the lazy\"\n",
    "print(f\"Original prompt: '{prompt}'\")\n",
    "\n",
    "# Run the model and get both the final output (logits) and a cache of all internal activations.\n",
    "original_logits, cache = model.run_with_cache(prompt)\n",
    "\n",
    "# The logits tensor has shape [batch_size, sequence_position, vocabulary_size]\n",
    "print(\"Logits tensor shape:\", original_logits.shape)\n",
    "\n",
    "# Let's find the model's prediction for the *next* token\n",
    "last_token_logits = original_logits[0, -1, :]\n",
    "predicted_token_index = torch.argmax(last_token_logits).item()\n",
    "predicted_token = model.to_string([predicted_token_index])\n",
    "\n",
    "print(f\"Model's top prediction for the next word: '{predicted_token}'\")\n",
    "\n",
    "# The 'cache' is a dictionary mapping activation names to their values\n",
    "# Let's see what the activation of the query vector of head 7 in layer 4 looks like.\n",
    "# The format is utils.get_act_name(activation_name, layer_index)\n",
    "key_to_check = transformer_lens.utils.get_act_name(\"q\", 4)\n",
    "activation_shape = cache[key_to_check][0, :, 7, :].shape # Batch, Seq Pos, Head Index, Head Dim\n",
    "\n",
    "print(f\"\\nShape of query vector for layer 4, head 7: {activation_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade3eec",
   "metadata": {},
   "source": [
    "### 4. Running an Ablation Experiment\n",
    "\n",
    "**Ablation** is the process of removing or disabling a part of the model to see how it affects the output. This helps us understand what different components (like an attention head) are responsible for.\n",
    "\n",
    "Here, we will \"zero-ablate\" a specific attention head. We'll write a **hook function** that intercepts the head's activation during the forward pass and replaces it with zeros. We can then see how this changes the model's prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d573e1",
   "metadata": {},
   "source": [
    "# We will target layer 8, attention head 10.\n",
    "# This head is known to be involved in identifying patterns of repeated tokens.\n",
    "LAYER_TO_ABLATE = 8\n",
    "HEAD_TO_ABLATE = 10\n",
    "\n",
    "def zero_ablate_head_hook(\n",
    "    activation_value: torch.Tensor,\n",
    "    hook: transformer_lens.hook_points.HookPoint\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    This function is our hook. It takes an activation and a hook object.\n",
    "    It modifies the activation in-place to zero out a specific head.\n",
    "    \"\"\"\n",
    "    print(f\"Ablating Layer {LAYER_TO_ABLATE}, Head {HEAD_TO_ABLATE}...\")\n",
    "    # Shape of activation_value: [batch, seq_pos, head_index, d_head]\n",
    "    activation_value[:, :, HEAD_TO_ABLATE, :] = 0.\n",
    "    return activation_value\n",
    "\n",
    "# The 'add_hook' method returns a new set of logits.\n",
    "# It runs the model with our custom hook function applied at the specified activation point.\n",
    "ablated_logits = model.run_with_hooks(\n",
    "    prompt,\n",
    "    fwd_hooks=[(transformer_lens.utils.get_act_name(\"z\", LAYER_TO_ABLATE), zero_ablate_head_hook)]\n",
    ")\n",
    "\n",
    "# Find the ablated model's prediction\n",
    "ablated_last_token_logits = ablated_logits[0, -1, :]\n",
    "ablated_predicted_token_index = torch.argmax(ablated_last_token_logits).item()\n",
    "ablated_predicted_token = model.to_string([ablated_predicted_token_index])\n",
    "\n",
    "print(f\"\\nOriginal prediction: '{predicted_token}'\")\n",
    "print(f\"Prediction after ablating L{LAYER_TO_ABLATE}H{HEAD_TO_ABLATE}: '{ablated_predicted_token}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097bdc71",
   "metadata": {},
   "source": [
    "### 5. Quantifying the Effect\n",
    "\n",
    "We can see the prediction changed! To measure this change more formally, we can calculate the **KL Divergence** between the original probability distribution (from the original logits) and the ablated one. A higher KL divergence means the ablation had a bigger impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a4ace9",
   "metadata": {},
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_log_probs(logits):\n",
    "    # Use the last token's logits for prediction\n",
    "    last_token_logits = logits[0, -1, :]\n",
    "    return F.log_softmax(last_token_logits, dim=-1)\n",
    "\n",
    "original_log_probs = get_log_probs(original_logits)\n",
    "ablated_log_probs = get_log_probs(ablated_logits)\n",
    "\n",
    "# KL Divergence D_KL(original || ablated)\n",
    "# Measures how much information is lost when using the ablated distribution to approximate the original.\n",
    "kl_div = F.kl_div(\n",
    "    input=ablated_log_probs, # The approximation Q\n",
    "    target=original_log_probs, # The true distribution P\n",
    "    log_target=True,\n",
    "    reduction=\"sum\"\n",
    ")\n",
    "\n",
    "print(f\"KL Divergence between original and ablated distributions: {kl_div.item():.4f}\")\n",
    "print(\"\\nA higher KL divergence indicates the ablated component was more important for the original prediction.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46abd29",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "You've successfully used TransformerLens to:\n",
    "- Load a model.\n",
    "- Inspect its internal activations.\n",
    "- Run an ablation experiment by zeroing out a specific attention head.\n",
    "- Quantify the impact of this change.\n",
    "\n",
    "This is a core workflow in mechanistic interpretability. From here, you can explore ablating different components, patching activations from one run to another, and much more!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b30075d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
